# -*- coding: utf-8 -*-
"""DataMiningGroupProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RSUrbUuhfx-GJasWyR-SIbckqF30VrSC

## **WQD 7005 Data Mining Group Project**

**Proejct Tittle: Housing Loan Prediction Model For Married Couples**

Group Member:
1.   NUR SHAFIQAH BINTI MOHAMAD JOHARI (22119564â€‹)
2.   LAW JIA JIN (22071390)
3.   LIM SZE SING (22109557)
4.   GAN JING WEN (22065433)


Methodology: SEMMA

Objection:
1. To identify the factors that will affects the housing loan approval for married couples

2. To develop a prediction model for housing loan of this population

3. To evaluate the performance of the model

# **1. Sample** #

Data source: https://github.com/naveen-chauhan/Loan-Prediction-Classification/tree/master/Dataset
"""

# Commented out IPython magic to ensure Python compatibility.
# linear algebra
import numpy as np

# data processing
import pandas as pd

# data visualization
import seaborn as sns
# %matplotlib inline
from matplotlib import pyplot as plt
from matplotlib import style

# Modelling
from sklearn.model_selection import train_test_split

#Hyperparameter tuning
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

# Algorithms
from sklearn import linear_model
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import Perceptron
from sklearn.linear_model import SGDClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.naive_bayes import GaussianNB
from tensorflow.keras.models import Sequential  #AAN
from tensorflow.keras.layers import Dense       #AAN

# Evaluation
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, roc_curve, confusion_matrix
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_predict
from sklearn.model_selection import GridSearchCV

train_df= pd.read_csv('/content/train.csv',sep=",")
test_df= pd.read_csv('/content/train.csv',sep=",")

#from IPython.display import display
#train_df = train_df.head().style.set_table_styles([{'selector': 'th', 'props': [('max-width', '100px')]}])
#display(train_df)
train_df.head(5)

#Summry of df1
train_df.info()

# Count the number of rows in df1
row_count = train_df.shape[0]
print("Number of rows:", row_count)

# Filter DataFrame, Married= Yes
#df = df[df1['Married'] == 'Yes']
train_df = train_df[train_df['Married'] == 'Yes']

#df_head = df.head().style.set_table_styles([{'selector': 'th', 'props': [('max-width', '100px')]}])
#display(df_head)
train_df.head(5)

# Count the number of rows in df
row_count = train_df.shape[0]
print("Number of rows:", row_count)

"""# **2. Explore**

## **2.1 Exploring Data Distribution**
"""

#Get general insights of data set

# Get the number of rows and columns
num_rows, num_columns = train_df.shape

# Print the number of rows and columns
print("Number of rows:", num_rows)
print("Number of columns:", num_columns)
train_df.info()
train_df.head(10)

"""From here we can conclude that

*   There are 398 rows and 13 columns available in the dataset
*   Combination of categorical and numerical values are available

## **2.2 Univariate Analysis - Numerical Variables**
"""

#Descriptive statistics insights, get the central tendencies
train_df.describe()

#Visualize statistics to histogram
train_df.hist(bins = 25,figsize=(10,10));

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# List of numerical columns
numerical_columns = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History']

# Creating subplots
fig, axes = plt.subplots(1, len(numerical_columns), figsize=(18, 4))

# Plotting box plots
for i, column in enumerate(numerical_columns):
    sns.boxplot(x=train_df[column], ax=axes[i], color='skyblue')
    axes[i].set_xlabel(column)
    axes[i].set_title(f'Box Plot of {column}')

plt.tight_layout()
plt.show()

"""From here we can conclude that

*   There are 5 columns identified with numerical values
*   ApplicantIncome, CoapplicantIncome and LoanAmount values are heavily skewed and have a lot of outliers. Data modification is needed to reduce biasness and skewness
* Loan_Amount_Term are spread out widely with no significant clustering suggesting that there is significant noise or randomness present in the data
* Credit_History has only 2 values which indicates a binary valued data. This type of data can simplify the model's prediction especially for regression but will need to handle class imbalances

## **2.3 Univariate Analysis - Categorical Variables**

Since we have already identified the numerical values, we can assume the rest of the columns are categorical valued columns.
"""

# Select columns containing categorical data
categorical_columns = ['Gender', 'Dependents', 'Education', 'Self_Employed', 'Property_Area', 'Loan_Status']

# Print unique values in each categorical column
for column in categorical_columns:
    unique_values = train_df[column].unique()
    print(f"Unique values in '{column}': {unique_values}")

# Count the frequency of each category in categorical columns
for column in categorical_columns:
    category_counts = train_df[column].value_counts()
    print(f"\nValue counts for '{column}':\n{category_counts}")

#Visualize categorical data

# Set up the matplotlib figure
plt.figure(figsize=(7, 10))

# Plot each categorical column
for i, column in enumerate(categorical_columns, 1):
    plt.subplot(3, 3, i)
    sns.countplot(data=train_df, x=column)
    plt.title(f'{column} Distribution')
    plt.xlabel(column)
    plt.ylabel('Count')
    plt.xticks(rotation=45)

# Adjust layout
plt.tight_layout()
plt.show()

"""From here we can conclude that

* Most categorical columns, other than Property_Area, seemed to have binary values where we can change it to numerical to standardize with other columns and studies can be easily done
* Loan_Status can be identified as the target variable in this study
* Since we are narrowing down our study case to married people & systematic sampling has been done, Married column can be removed altogether for the study
* Data distribution for Gender and Self_Employed are quite imbalanced. It is important to take consideration of potential biasness througout the study.

## **2.3 Bivariate Analysis**

In order for us to do Bivariate or Multivariate analysis, its better to have all categorical variables be encoded to numerical ones. Since we do have a variable that has more than 2 unique instances, we will proceed with label encoding.

Also, it was also identified that Married column is no longer needed for the study hence we will drop it.
"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Make a copy of the DataFrame
encoded_df = train_df.copy()

# Drop the 'Married' column
encoded_df.drop(['Married', 'Loan_ID'], axis=1, inplace=True)

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Iterate over all remaining columns
for col in encoded_df.columns:
    # Check if the column is of object type (categorical)
    if encoded_df[col].dtype == 'object':
        # Encode categorical columns
        encoded_df[col] = label_encoder.fit_transform(encoded_df[col])

print(encoded_df.head())

"""Using Scatter Plot to investigate the relation between target variable and all variables :"""

# Define your target variable
target_variable = 'Loan_Status'

# Get list of all variables (including target variable)
all_variables = encoded_df.columns

# Remove the target variable from the list of other variables
other_variables = [col for col in all_variables if col != target_variable]

# Determine the number of rows and columns for subplots
num_rows = len(other_variables) // 3 + (len(other_variables) % 3 > 0)
num_cols = min(len(other_variables), 4)

# Create subplots
fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows))

# Plot scatter plots with trendline for each variable with respect to the target variable
for i, variable in enumerate(other_variables):
    row = i // num_cols
    col = i % num_cols

    # Plot scatter plot with trendline
    sns.regplot(x=variable, y=target_variable, data=encoded_df, ax=axes[row, col], scatter_kws={'s': 20})
    axes[row, col].set_xlabel(variable)
    axes[row, col].set_ylabel(target_variable)
    axes[row, col].set_title(f'{variable} vs {target_variable}')

# Remove any empty subplots
for i in range(len(other_variables), num_rows * num_cols):
    row = i // num_cols
    col = i % num_cols
    fig.delaxes(axes[row, col])

plt.tight_layout()
plt.show()

"""From here we can conclude that

* Loan_ID is not necessary to the study since all inputs are unique
* Not all variables have exponential value to the target variable and outliers are prominent
* Data cleaning is crucial in order to get a more precise plotting

## **2.5 Multivariate Analysis**
"""

# Calculate correlation
correlation = encoded_df.corr()

# Plot heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Heatmap')
plt.show()

"""From here we can conclude that

* Credit History has a high correlation to Loan_Status
* Property_Area, Loan_Amount_Term and Self_Employed have correlation to Loan_Status but rather low
* Other variables have no correlation to Loan_Status, however this might change once we clean the data

## **2.6 Data Quality**

From here we will have a check on the overall quality of the data. This process will be done as below :


*   Examine missing values
*   Examine duplicated rows
"""

#Examine missing values
encoded_df.isnull().sum()

encoded_df.duplicated().sum()

"""## **2.7 Conclusion**



1.   Dataset consist of both numerical and categorical values, encoding data is a crucial step for us to proceed with modeling step
2. Targeted variable can be identified as Loan_Status for the study, while other potential variables can be further identified after data modification
3. As for current stage, it seems like Credit History, Property_Area, Loan_Amount_Term and Self_Employed have correlation to Loan_Status however further checking is necessary after data modification
4. Dataset overall is heavily skewed, prominent outliers, and contains missing values. Necessary data modification is needed to ensure study will go smooth

# **3. Modify**
"""

train_df.info()

train_df.describe().T

"""From here we can see that ApplicantIncome & CoapplicantIncome std is higher than mean which is abnormal. This indicate the dataset might be heavily skewed or have outlier.

Other than that, the minimum loan amount is 17, which is far from the mean. This need to check further on the loan amount term, if says the loan amount is 17 but the loan amount term is 360, meaning the installment amount for one team is 0.047 which is too little. If this is the case, this row of data we shall consider as error data.

Furthermore, the minimum Loan_Amount_Term is only 12 which is far from the mean. This could also be another error data.

## 3.1 Dropping unnecessary columns

First, we will drop Loan_ID from the train set and test, because it does not contribute to a loan approval status.
"""

#Drop feature Loan_ID
train_df = train_df.drop(['Loan_ID'], axis=1)
test_df = test_df.drop(['Loan_ID'], axis=1)

"""Next, we will drop Married feature from the train and test set since we already did systematic sampling to include only married data.  """

#Drop feature Married
train_df = train_df.drop(['Married'], axis=1)
test_df = test_df.drop(['Married'], axis=1)

"""##3.2 Handle missing data





"""

#Check the missing data
total = train_df.isnull().sum().sort_values(ascending=False)
percent_1 = train_df.isnull().sum()/train_df.isnull().count()*100
percent_2 = (round(percent_1, 1)).sort_values(ascending=False)
missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])
missing_data.head(7)

"""To handle the missing data:


*   Loan_Amount_Term and Dependents - fill with mode
*   LoanAmount - fill with mean
*   Self_Employed, Credit_History, Gender - fill with random number.
"""

#Fill missing data with mode
train_df['Loan_Amount_Term'].fillna(train_df['Loan_Amount_Term'].mode()[0], inplace=True)
test_df['Loan_Amount_Term'].fillna(test_df['Loan_Amount_Term'].mode()[0], inplace=True)

#Fill missing data with mode
train_df['Dependents'].fillna(train_df['Dependents'].mode()[0], inplace=True)
test_df['Dependents'].fillna(test_df['Dependents'].mode()[0], inplace=True)

#Fill missing data with mean
train_df['LoanAmount'].fillna(train_df['LoanAmount'].mean(), inplace=True)
test_df['LoanAmount'].fillna(test_df['LoanAmount'].mean(), inplace=True)

# Function to fill missing data with random number
def fill_missing_with_random(df, column):
    existing_values = df[column].dropna().unique()
    df[column] = df[column].apply(lambda x: np.random.choice(existing_values) if pd.isnull(x) else x)

#Fill missing data with random number
fill_missing_with_random(train_df, 'Self_Employed')
fill_missing_with_random(test_df, 'Self_Employed')

#Fill missing data with random number
fill_missing_with_random(train_df, 'Credit_History')
fill_missing_with_random(test_df, 'Credit_History')

#change the data type to integer
train_df['Credit_History'] = train_df['Credit_History'].astype(int)
test_df['Credit_History'] = test_df['Credit_History'].astype(int)

#Fill missing data with random number
fill_missing_with_random(train_df, 'Gender')
fill_missing_with_random(test_df, 'Gender')

"""## 3.3 Transform Categorical Data

Next, we will convert 'Gender', 'Dependents','Education','Self-Employed','Property_Area','Loan_Status' feature from categorical into numeric.
"""

genders = {"Male": 0, "Female": 1}
data = [train_df, test_df]

for dataset in data:
    dataset['Gender'] = dataset['Gender'].map(genders)

dependents = {"0": 0, "1": 1, "2": 2, "3+": 3}
data = [train_df, test_df]

for dataset in data:
    dataset['Dependents'] = dataset['Dependents'].map(dependents)

educations = {"Not Graduate": 0, "Graduate": 1}
data = [train_df, test_df]

for dataset in data:
    dataset['Education'] = dataset['Education'].map(educations)

self_employeds = {"No": 0, "Yes": 1}
data = [train_df, test_df]

for dataset in data:
    dataset['Self_Employed'] = dataset['Self_Employed'].map(self_employeds)

property_areas = {"Urban": 0, "Semiurban": 1, "Rural":2}
data = [train_df, test_df]

for dataset in data:
    dataset['Property_Area'] = dataset['Property_Area'].map(property_areas)

loan_status = {"N": 0, "Y": 1}
train_df['Loan_Status'] = train_df['Loan_Status'].map(loan_status)

train_df.info()

train_df.head(5)

"""## 3.4 Column Concatenation

Next, we will add one column named 'TotalIncome'=ApplicantIncome+CoapplicantIncome

TotalIncome we believe will be one of the important feature for the loan approval.
"""

train_df['TotalIncome'] = train_df['ApplicantIncome'] + train_df['CoapplicantIncome']
test_df['TotalIncome'] = test_df['ApplicantIncome'] + test_df['CoapplicantIncome']

"""Next, we will also add one column named 'InstallmentAmount'=LoanAmount/Loan_Amount_Term. This is to detect is there any error data where the installment amount doesn't make sense. For now we doesn't take interest into account.

For example, if says the loan amount is 9 but the loan amount term is 360. Meaning the installment for one term is 0.025 which is too little and hence we can conclude this row of data is error data.
"""

train_df['InstallmentAmount'] = train_df['LoanAmount'] / train_df['Loan_Amount_Term']
test_df['InstallmentAmount'] = test_df['LoanAmount'] / test_df['Loan_Amount_Term']

"""## 3.5 Handle Outliers"""

fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(14, 8))

# List of column names you want to plot
columns = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term','TotalIncome', 'InstallmentAmount']

counter = 0

for i in range(2):
    for j in range(3):
        sns.boxplot(data=train_df[columns[counter]], ax=axs[i, j])
        axs[i, j].set_title(columns[counter].capitalize())
        counter += 1

plt.tight_layout()
plt.show()

"""We can see there're outlier for ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount_Term, TotalIncome, InstallmentAmount features but these outliers represent natural variations in the population, and they should be left as is in dataset. These are called true outliers. Hence, we will not remove the outliers."""

train_df.describe()

"""We can see the InstallmentAmount, the min is 0
072222 and max is 9.25 which is very far from the mean, hence these data are the error data. Let's take a look which row of data have error data.
"""

# Filter the DataFrame where InstallmentAmount is equal to 9.25
filtered_df = train_df[(train_df['InstallmentAmount'] == 0.072222) | (train_df['InstallmentAmount'] == 9.250000)]

# Display the filtered DataFrame as a table
print("Rows where InstallmentAmount is equal to 0.072222 and 9.250000:")
print(filtered_df)

"""For Row 497: LoanAmount=111 and Loan_Amount_Term is only 12. --> LoanAmount fall within 25% quartile and 75% quartile but Loan_Amount_Term is abnormal.

We'll remove this row.
"""

# Remove the rows where InstallmentAmount is equal to 0.025000 or 9.250000 from the original DataFrame
train_df = train_df[(train_df['InstallmentAmount'] != 0.072222) & (train_df['InstallmentAmount'] != 9.250000)]

train_df.info()

"""Now the dataset left with 397 entries."""

#Since we have derive TotalIncome and InstallmentAmount, we can drop ApplicantIncome, CoapplicantIncome, Loan_Amount_Term, LoanAmount
train_df = train_df.drop(['ApplicantIncome', 'CoapplicantIncome','Loan_Amount_Term','LoanAmount'], axis=1)
test_df = test_df.drop(['ApplicantIncome', 'CoapplicantIncome','Loan_Amount_Term','LoanAmount'], axis=1)

"""Next, we will categorizing TotalIncome into a group. We would need to make sure the group is distributeed evenly, we don't want the 80% of the data falls into one group."""

plt.figure(figsize=(8, 6))
sns.histplot(train_df['TotalIncome'], kde=True, bins=5, color='skyblue')
plt.title('Histogram of TotalIncome')
plt.xlabel('TotalIncome')
plt.ylabel('Frequency')
plt.show()

data = [train_df, test_df]
for dataset in data:
    dataset.loc[dataset['TotalIncome'] < 1000, 'TotalIncome'] = 0
    dataset.loc[(dataset['TotalIncome'] >= 1001) & (dataset['TotalIncome'] <= 2000), 'TotalIncome'] = 1
    dataset.loc[(dataset['TotalIncome'] >= 2001) & (dataset['TotalIncome'] <= 3000), 'TotalIncome'] = 2
    dataset.loc[(dataset['TotalIncome'] >= 3001) & (dataset['TotalIncome'] <= 4000), 'TotalIncome'] = 3
    dataset.loc[(dataset['TotalIncome'] >= 4001) & (dataset['TotalIncome'] <= 5000), 'TotalIncome'] = 4
    dataset.loc[(dataset['TotalIncome'] >= 5001) & (dataset['TotalIncome'] <= 6000), 'TotalIncome'] = 5
    dataset.loc[(dataset['TotalIncome'] >= 6001) & (dataset['TotalIncome'] <= 7000), 'TotalIncome'] = 6
    dataset.loc[(dataset['TotalIncome'] >= 7001) & (dataset['TotalIncome'] <= 8000), 'TotalIncome'] = 7
    dataset.loc[(dataset['TotalIncome'] >= 8001) & (dataset['TotalIncome'] <= 9000), 'TotalIncome'] = 8
    dataset.loc[(dataset['TotalIncome'] >= 9001) & (dataset['TotalIncome'] <= 10000), 'TotalIncome'] = 9
    dataset.loc[(dataset['TotalIncome'] >= 10001) & (dataset['TotalIncome'] <= 12000), 'TotalIncome'] = 10
    dataset.loc[(dataset['TotalIncome'] >= 12001) & (dataset['TotalIncome'] <= 14000), 'TotalIncome'] = 11
    dataset.loc[(dataset['TotalIncome'] >= 14001) & (dataset['TotalIncome'] <= 16000), 'TotalIncome'] = 12
    dataset.loc[ dataset['TotalIncome'] >= 16001, 'TotalIncome'] = 13

train_df['TotalIncome'].value_counts()

train_df.head(5)

"""## 3.6 Correlation Analysis"""

# Calculate correlation
correlation = train_df.corr()

# Plot heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Heatmap')
plt.show()

# Compute the correlation matrix
corr_matrix = train_df.corr()

# Get the absolute correlation values of features with the target column
corr_with_target = corr_matrix['Loan_Status'].abs().sort_values()

# Select the top 5 columns with the lowest correlation with the target column
top_5_low_corr_columns = corr_with_target[:5]

# Print the top 5 columns with the lowest correlation
print("Top 5 columns with the lowest correlation with the target column (Loan_Status):\n", top_5_low_corr_columns)

"""TotalIncome and Gender is having the lowest correlation with the target column, hence dropping this feature."""

train_df = train_df.drop(['Gender'], axis=1)
test_df = test_df.drop(['Gender'], axis=1)

train_df = train_df.drop(['TotalIncome'], axis=1)
test_df = test_df.drop(['TotalIncome'], axis=1)

test_df['Loan_Status'] = test_df['Loan_Status'].map({'Y': 1, 'N': 0})

"""# **4. Model**

Next, 5 different models are build and test with test set to predit result after training the model using train set.
The models are:
1. Logistic Regression
2. Naive Bayes
3. Random Forest
4. Artificial Neural Network
5. Supprt Vector Machine (SVM)

For each model, we'll follow a systematic approach:
1. Train the Model: Fit the model to the training data.
2. Test the Model: Predict the results using the test data.

3. Evaluate Performance: Calculate accuracy, and examine the confusion matrix and classification report.
"""

test_df.info()

train_df.info()

X_train = train_df.drop('Loan_Status',axis = 1)
y_train = train_df['Loan_Status']

X_test = test_df.drop('Loan_Status',axis = 1)
y_test = test_df['Loan_Status']

#check
import pandas as pd
pd.set_option('display.max_columns', None)  # To display all columns

# Assuming X_train is a DataFrame
nan_counts = X_train.isnull().sum()
print(nan_counts)

"""## 4.1 Logistic Regression"""

model_LR = LogisticRegression()
model_LR.fit(X_train, y_train)
y_pred_LR = model_LR.predict(X_test)

accuracy_LR = accuracy_score(y_test, y_pred_LR)
print("Accuracy of Logistic Regression Model:",accuracy_LR)
print(confusion_matrix(y_test, y_pred_LR))
print(classification_report(y_test, y_pred_LR))

"""## 4.2 Gaissian Naive Bayes"""

Model_NB = GaussianNB()
Model_NB.fit(X_train,y_train)
y_pred_NB = Model_NB.predict(X_test)

accuracy_NB = accuracy_score(y_test,y_pred_NB)
print("Accuracy of Naive Bayes Model:", accuracy_NB)
print(confusion_matrix(y_test,y_pred_NB))
print(classification_report(y_test,y_pred_NB))

"""## 4.3 Random Forest"""

Model_RF = RandomForestClassifier()
Model_RF.fit(X_train, y_train)
y_pred_RF = Model_RF.predict(X_test)

accuracy_RF = accuracy_score(y_test, y_pred_RF)
print("Accuracy of Random Forest Model:", accuracy_RF)
print(confusion_matrix(y_test, y_pred_RF))
print(classification_report(y_test, y_pred_RF))

"""## 4.4 Artificial Neural Network Model"""

# Define the model architecture
model_ann = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')
])

# Compile the model
model_ann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model_ann.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

# Make predictions
y_pred_prob_ann = model_ann.predict(X_test)
y_pred_ann = (y_pred_prob_ann > 0.5).astype(int)
y_test_binary = (y_test > 0.5).astype(int)

# Evaluate the model on test data
loss, accuracy_AAN = model_ann.evaluate(X_test, y_test)
print("Accuracy of Artificial Neural Network Model:", accuracy_AAN)
print(confusion_matrix(y_test, y_pred_ann))
print(classification_report(y_test, y_pred_ann))

"""## 4.5 Support Vector Machine"""

model_svm = SVC()
model_svm.fit(X_train, y_train)
y_pred_svm = model_svm.predict(X_test)

accuracy_SVM = accuracy_score(y_test, y_pred_svm)
print("Accuracy of Support Vector Machine (SVM) Model:", accuracy_SVM)
print(confusion_matrix(y_test, y_pred_svm))
print(classification_report(y_test, y_pred_svm))

"""# **5. Assess**

##  5.1 Confusion Matrix
"""

# Create a figure and axes for subplots
fig, axes = plt.subplots(1, 5, figsize=(20, 6))

# Confusion matrix for Logistic Regression
cm_LR = confusion_matrix(y_test, y_pred_LR)
sns.heatmap(cm_LR, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[0])
axes[0].set_title('Logistic Regression')
axes[0].set_xlabel('Predicted')
axes[0].set_ylabel('Actual')

# Confusion matrix for Naive Bayes
cm_NB = confusion_matrix(y_test, y_pred_NB)
sns.heatmap(cm_NB, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[1])
axes[1].set_title('Naive Bayes')
axes[1].set_xlabel('Predicted')
axes[1].set_ylabel('')

# Confusion matrix for Random Forest
cm_RF = confusion_matrix(y_test, y_pred_RF)
sns.heatmap(cm_RF, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[2])
axes[2].set_title('Random Forest')
axes[2].set_xlabel('Predicted')
axes[2].set_ylabel('')

# Confusion matrix for Artificial Neural Network (ANN)
cm_ann = confusion_matrix(y_test, y_pred_ann)
sns.heatmap(cm_ann, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[3])
axes[3].set_title('ANN')
axes[3].set_xlabel('Predicted')
axes[3].set_ylabel('')

# Confusion matrix for Support Vector Machine (SVM)
cm_svm = confusion_matrix(y_test, y_pred_svm)
sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[4])
axes[4].set_title('SVM')
axes[4].set_xlabel('Predicted')
axes[4].set_ylabel('')

plt.tight_layout()
plt.show()

"""## 5.2 Evaluation Matrix"""

# Compute evaluation metrics for each model
metrics = {
    'Model': ['Logistic Regression', 'Naive Bayes', 'Random Forest', 'Artificial Neural Network', 'Support Vector Machine (SVM)'],
    'Accuracy': [accuracy_LR, accuracy_NB, accuracy_RF, accuracy_AAN, accuracy_SVM],
    'Precision': [precision_score(y_test, y_pred_LR), precision_score(y_test, y_pred_NB), precision_score(y_test, y_pred_RF), precision_score(y_test, y_pred_ann), precision_score(y_test, y_pred_svm)],
    'Recall': [recall_score(y_test, y_pred_LR), recall_score(y_test, y_pred_NB), recall_score(y_test, y_pred_RF), recall_score(y_test, y_pred_ann), recall_score(y_test, y_pred_svm)],
    'F1 Score': [f1_score(y_test, y_pred_LR), f1_score(y_test, y_pred_NB), f1_score(y_test, y_pred_RF), f1_score(y_test, y_pred_ann), f1_score(y_test, y_pred_svm)],
    'ROC AUC': [roc_auc_score(y_test, y_pred_LR), roc_auc_score(y_test, y_pred_NB), roc_auc_score(y_test, y_pred_RF), roc_auc_score(y_test, y_pred_ann), roc_auc_score(y_test, y_pred_svm)]
}

# Create a DataFrame to display the metrics
metrics_df = pd.DataFrame(metrics)

# Display the DataFrame
print(metrics_df)

# Define the models and their corresponding performance metrics
models = ['Logistic Regression', 'Naive Bayes', 'Random Forest', 'Artificial Neural Network', 'Support Vector Machine (SVM)']
accuracy = [0.799674, 0.801303, 0.856678, 0.786645, 0.801303]
precision = [0.793713, 0.794118, 0.874439, 0.779271, 0.794118]
recall = [0.957346, 0.959716, 0.924171, 0.962085, 0.959716]
f1_score = [0.867884, 0.869099, 0.898618, 0.861082, 0.869099]
roc_auc = [0.705235, 0.706420, 0.816252, 0.681563, 0.706420 ]
# Set width of bar
bar_width = 0.15
# Set position of bar on X axis
r1 = range(len(accuracy))
r2 = [x + bar_width for x in r1]
r3 = [x + bar_width for x in r2]
r4 = [x + bar_width for x in r3]
r5 = [x + bar_width for x in r4]
# Make the plot
plt.figure(figsize=(10, 6))
plt.bar(r1, accuracy, color='b', width=bar_width, edgecolor='grey', label='Accuracy')
plt.bar(r2, precision, color='g', width=bar_width, edgecolor='grey', label='Precision')
plt.bar(r3, recall, color='r', width=bar_width, edgecolor='grey', label='Recall')
plt.bar(r4, f1_score, color='c', width=bar_width, edgecolor='grey', label='F1 Score')
plt.bar(r5, roc_auc, color='m', width=bar_width, edgecolor='grey', label='ROC AUC')

# Add xticks on the middle of the group bars
plt.xlabel('Models', fontweight='bold')
plt.xticks([r + bar_width*2 for r in range(len(accuracy))], models, rotation=45)
# Add ylabel
plt.ylabel('Scores', fontweight='bold')
# Create legend & Show graphic
plt.legend()
plt.title('Performance Metrics of Different Models')
plt.tight_layout()
plt.show()

"""Overall based on the evaluation matrix:

The Random Forest model outperforms others in terms of accuracy, precision, recall, and F1-score.

  *   has the highest accuracy (0.85) among all models, indicating that it makes the most correct predictions overall.
  *    has the highest precision (0.87), recall (0.92), and F1 score (0.89), suggesting that it strikes a good balance between precision and recall, making it suitable for imbalanced datasets.
  *   has the highest ROC AUC (0.81), indicating its ability to discriminate between true positives and false positives.

Random Forest appears to be the best-performing model for loan prediction in this comparison.

## 5.3 Hyperparameter tuning
"""

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

# Define the hyperparameter distributions
param_dist = {
    'n_estimators': randint(50, 200),  # Randomly sample integers from 50 to 200
    'max_depth': [None] + list(range(5, 30, 5)),  # Include None and sample from 5 to 25 with step 5
    'min_samples_split': randint(2, 11),  # Randomly sample integers from 2 to 10
    'min_samples_leaf': randint(1, 5)  # Randomly sample integers from 1 to 4
}

# Initialize Random Forest classifier
rf_classifier = RandomForestClassifier()

# Perform random search with cross-validation
random_search = RandomizedSearchCV(rf_classifier, param_distributions=param_dist, n_iter=100, cv=5, scoring='accuracy', random_state=42)
random_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = random_search.best_params_
print("Best Hyperparameters:", best_params)

# Retrain the model with the best hyperparameters
best_rf_model = RandomForestClassifier(**best_params)
best_rf_model.fit(X_train, y_train)

# Evaluate the model on the test set
y_pred_test = best_rf_model.predict(X_test)
accuracy_test = accuracy_score(y_test, y_pred_test)
print("Accuracy on Test Set:", accuracy_test)

"""##5.4 Cross Validation (Random Forest)"""

# Perform 5-fold cross-validation
cv_scores = cross_val_score(best_rf_model, X_train, y_train, cv=5)
# Print the cross-validation scores
print("Cross-validation scores:", cv_scores)
# Calculate the mean and standard deviation of the cross-validation scores
print("Mean CV score:", np.mean(cv_scores))
print("Standard deviation of CV scores:", np.std(cv_scores))

"""*   On average, the model achieves an accuracy of around 80.85% across different subsets of the training data.
*   The standard deviation is approximately 0.0348, suggesting relatively low variability in the model's performance

##5.5 Feature Importance Analysis
"""

# Get feature importances
importances = best_rf_model.feature_importances_
# Get the column names (feature names) exclude loan_status
feature_names = train_df.columns[:-2].tolist() +train_df.columns[-1:].tolist()
# Get the indices of features sorted by importance
indices = np.argsort(importances)[::-1]
# Print the feature ranking
print("Feature ranking:")
for f in range(len(feature_names)):
    print("%d. Feature %s (%f)" % (f + 1, feature_names[indices[f]], importances[indices[f]]))
# Convert indices to a list
indices_list = list(indices)
# Plot the feature importances
plt.figure(figsize=(10, 6))
plt.title("Feature Importances")
plt.bar(range(len(feature_names)), importances[indices], color="b", align="center")
plt.xticks(range(len(feature_names)), [feature_names[i] for i in indices_list], rotation=90)
plt.xlabel("Feature")
plt.ylabel("Importance Score")
plt.show()

"""Credit_History (0.549542) has the highest importance score, indicating that it plays a significant role in predicting the target variable (loan status)."""